{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12e957d3",
   "metadata": {},
   "source": [
    "# Advanced RAG Exploration üî¨\n",
    "\n",
    "This notebook explores cutting-edge Retrieval-Augmented Generation (RAG) techniques.\n",
    "\n",
    "## Exploration Areas\n",
    "- Hierarchical document chunking\n",
    "- Query decomposition strategies\n",
    "- Contextual compression\n",
    "- Multi-modal retrieval\n",
    "- Performance optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2bbaeb",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39dff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Add utils to path\n",
    "sys.path.append(str(Path.cwd().parent / 'utils'))\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Configuration\n",
    "try:\n",
    "    from utils.config import get_api_key\n",
    "    api_key = get_api_key('openai')\n",
    "    DEMO_MODE = not api_key\n",
    "except ImportError:\n",
    "    api_key = os.getenv('OPENAI_API_KEY')\n",
    "    DEMO_MODE = not api_key\n",
    "\n",
    "print(f\"üî¨ Advanced RAG Exploration Lab\")\n",
    "print(f\"Demo mode: {DEMO_MODE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a16d5e",
   "metadata": {},
   "source": [
    "## Experiment 1: Hierarchical Document Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116f930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalChunker:\n",
    "    \"\"\"Advanced chunking that preserves document hierarchy.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.chunk_sizes = [2000, 1000, 500]\n",
    "        self.overlap = 100\n",
    "    \n",
    "    def chunk_with_hierarchy(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Create hierarchical chunks with metadata.\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # Large chunks for context\n",
    "        large_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_sizes[0], chunk_overlap=self.overlap\n",
    "        )\n",
    "        large_chunks = large_splitter.split_text(text)\n",
    "        \n",
    "        for i, large_chunk in enumerate(large_chunks):\n",
    "            # Small chunks for precise retrieval\n",
    "            small_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=self.chunk_sizes[2], chunk_overlap=self.overlap\n",
    "            )\n",
    "            small_chunks = small_splitter.split_text(large_chunk)\n",
    "            \n",
    "            for j, small_chunk in enumerate(small_chunks):\n",
    "                chunks.append({\n",
    "                    'content': small_chunk,\n",
    "                    'chunk_id': f\"{i}-{j}\",\n",
    "                    'parent_context': large_chunk[:200] + \"...\",\n",
    "                    'size': len(small_chunk)\n",
    "                })\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# Test hierarchical chunking\n",
    "sample_text = \"\"\"\n",
    "Artificial Intelligence (AI) is transforming industries worldwide.\n",
    "\n",
    "Machine Learning, a subset of AI, enables computers to learn from data without explicit programming.\n",
    "Deep Learning uses neural networks to solve complex problems.\n",
    "Natural Language Processing helps computers understand human language.\n",
    "\n",
    "Applications include healthcare diagnostics, autonomous vehicles, and financial fraud detection.\n",
    "\"\"\" * 5\n",
    "\n",
    "chunker = HierarchicalChunker()\n",
    "hierarchical_chunks = chunker.chunk_with_hierarchy(sample_text)\n",
    "\n",
    "print(f\"üìä Hierarchical Chunking Results:\")\n",
    "print(f\"Total chunks: {len(hierarchical_chunks)}\")\n",
    "print(f\"Average chunk size: {np.mean([chunk['size'] for chunk in hierarchical_chunks]):.0f} chars\")\n",
    "\n",
    "# Show sample chunks\n",
    "print(\"\\nüîç Sample chunks:\")\n",
    "for i, chunk in enumerate(hierarchical_chunks[:2]):\n",
    "    print(f\"Chunk {chunk['chunk_id']}: {chunk['content'][:100]}...\")\n",
    "    print(f\"Context: {chunk['parent_context'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a4b180",
   "metadata": {},
   "source": [
    "## Experiment 2: Query Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c818f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryDecomposer:\n",
    "    \"\"\"Decompose complex queries into simpler sub-queries.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def decompose_query(self, query: str) -> List[str]:\n",
    "        \"\"\"Break down complex query into simpler questions.\"\"\"\n",
    "        if DEMO_MODE:\n",
    "            # Mock decomposition\n",
    "            if \"machine learning\" in query.lower():\n",
    "                return [\n",
    "                    \"What is machine learning?\",\n",
    "                    \"How does machine learning work?\",\n",
    "                    \"What are machine learning applications?\"\n",
    "                ]\n",
    "            return [query]\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "Break down this query into 3 simpler questions:\n",
    "Query: {query}\n",
    "\n",
    "Questions:\n",
    "1.\n",
    "2.\n",
    "3.\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.predict(prompt)\n",
    "            questions = []\n",
    "            for line in response.split('\\n'):\n",
    "                if line.strip() and line[0].isdigit():\n",
    "                    question = line.split('.', 1)[-1].strip()\n",
    "                    if len(question) > 10:\n",
    "                        questions.append(question)\n",
    "            return questions[:3]\n",
    "        except:\n",
    "            return [query]\n",
    "\n",
    "# Test query decomposition\n",
    "llm = ChatOpenAI(openai_api_key=api_key, temperature=0) if not DEMO_MODE else None\n",
    "decomposer = QueryDecomposer(llm)\n",
    "\n",
    "test_query = \"How does machine learning work and what are its applications?\"\n",
    "sub_queries = decomposer.decompose_query(test_query)\n",
    "\n",
    "print(f\"üß™ Query Decomposition:\")\n",
    "print(f\"Original: {test_query}\")\n",
    "print(\"Decomposed into:\")\n",
    "for i, sq in enumerate(sub_queries, 1):\n",
    "    print(f\"  {i}. {sq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66010b9",
   "metadata": {},
   "source": [
    "## Experiment 3: Contextual Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d37792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualCompressor:\n",
    "    \"\"\"Compress documents to extract only relevant information.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def compress_document(self, query: str, document: str) -> str:\n",
    "        \"\"\"Extract only query-relevant information from document.\"\"\"\n",
    "        if DEMO_MODE:\n",
    "            # Mock compression - return first 200 chars\n",
    "            return document[:200] + \"... [compressed]\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "Extract only information relevant to this query:\n",
    "Query: {query}\n",
    "\n",
    "Document: {document}\n",
    "\n",
    "Relevant information:\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            return self.llm.predict(prompt)\n",
    "        except:\n",
    "            return document[:300]  # Fallback to truncation\n",
    "    \n",
    "    def score_relevance(self, query: str, document: str) -> float:\n",
    "        \"\"\"Score document relevance to query.\"\"\"\n",
    "        if DEMO_MODE:\n",
    "            # Mock scoring based on keyword overlap\n",
    "            query_words = set(query.lower().split())\n",
    "            doc_words = set(document.lower().split())\n",
    "            overlap = len(query_words & doc_words)\n",
    "            return min(overlap * 2.0, 10.0)\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "Rate relevance (1-10) of this document to the query:\n",
    "Query: {query}\n",
    "Document: {document[:500]}\n",
    "\n",
    "Score (number only):\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.predict(prompt)\n",
    "            return float(''.join(filter(str.isdigit, response.split()[0])))\n",
    "        except:\n",
    "            return 5.0  # Default score\n",
    "\n",
    "# Test contextual compression\n",
    "compressor = ContextualCompressor(llm)\n",
    "\n",
    "sample_docs = [\n",
    "    \"Machine learning is a subset of AI that enables computers to learn from data.\",\n",
    "    \"Python is a programming language popular in data science and web development.\",\n",
    "    \"Deep learning uses neural networks to solve complex pattern recognition problems.\"\n",
    "]\n",
    "\n",
    "query = \"What is machine learning?\"\n",
    "\n",
    "print(f\"üîç Contextual Compression for: '{query}'\\n\")\n",
    "\n",
    "for i, doc in enumerate(sample_docs):\n",
    "    relevance = compressor.score_relevance(query, doc)\n",
    "    compressed = compressor.compress_document(query, doc)\n",
    "    \n",
    "    print(f\"Document {i+1} (Relevance: {relevance:.1f}/10):\")\n",
    "    print(f\"Original: {doc}\")\n",
    "    print(f\"Compressed: {compressed[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74e2ac9",
   "metadata": {},
   "source": [
    "## Experiment 4: Multi-Vector Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdd57f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiVectorRetriever:\n",
    "    \"\"\"Retrieval using multiple vector representations.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.documents = []\n",
    "        self.document_vectors = {}  # Simulated vector storage\n",
    "    \n",
    "    def add_document(self, doc: Document):\n",
    "        \"\"\"Add document with multiple vector representations.\"\"\"\n",
    "        self.documents.append(doc)\n",
    "        doc_id = len(self.documents) - 1\n",
    "        \n",
    "        # Simulate different vector types\n",
    "        self.document_vectors[doc_id] = {\n",
    "            'content_vector': self._mock_embedding(doc.page_content),\n",
    "            'summary_vector': self._mock_embedding(doc.page_content[:100]),\n",
    "            'keyword_vector': self._keyword_vector(doc.page_content)\n",
    "        }\n",
    "    \n",
    "    def _mock_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"Mock embedding based on text characteristics.\"\"\"\n",
    "        # Simple hash-based mock embedding\n",
    "        import hashlib\n",
    "        hash_val = int(hashlib.md5(text.encode()).hexdigest(), 16)\n",
    "        return [(hash_val >> i) % 100 / 100.0 for i in range(10)]\n",
    "    \n",
    "    def _keyword_vector(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"Create keyword-based vector.\"\"\"\n",
    "        words = text.lower().split()\n",
    "        word_counts = {}\n",
    "        for word in words:\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "        return word_counts\n",
    "    \n",
    "    def similarity_search(self, query: str, k: int = 3) -> List[Document]:\n",
    "        \"\"\"Search using multiple vector types.\"\"\"\n",
    "        query_vector = self._mock_embedding(query)\n",
    "        query_keywords = self._keyword_vector(query)\n",
    "        \n",
    "        doc_scores = []\n",
    "        \n",
    "        for doc_id, vectors in self.document_vectors.items():\n",
    "            # Combine different similarity scores\n",
    "            content_sim = self._cosine_similarity(query_vector, vectors['content_vector'])\n",
    "            keyword_sim = self._keyword_similarity(query_keywords, vectors['keyword_vector'])\n",
    "            \n",
    "            # Weighted combination\n",
    "            combined_score = 0.7 * content_sim + 0.3 * keyword_sim\n",
    "            doc_scores.append((doc_id, combined_score))\n",
    "        \n",
    "        # Sort by score and return top k\n",
    "        doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [self.documents[doc_id] for doc_id, _ in doc_scores[:k]]\n",
    "    \n",
    "    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:\n",
    "        \"\"\"Calculate cosine similarity.\"\"\"\n",
    "        dot_product = sum(a * b for a, b in zip(vec1, vec2))\n",
    "        norm1 = sum(a * a for a in vec1) ** 0.5\n",
    "        norm2 = sum(b * b for b in vec2) ** 0.5\n",
    "        return dot_product / (norm1 * norm2) if norm1 * norm2 > 0 else 0\n",
    "    \n",
    "    def _keyword_similarity(self, keywords1: Dict, keywords2: Dict) -> float:\n",
    "        \"\"\"Calculate keyword overlap similarity.\"\"\"\n",
    "        common_words = set(keywords1.keys()) & set(keywords2.keys())\n",
    "        if not common_words:\n",
    "            return 0.0\n",
    "        \n",
    "        overlap_score = sum(min(keywords1[word], keywords2[word]) for word in common_words)\n",
    "        total_words = sum(keywords1.values()) + sum(keywords2.values())\n",
    "        return overlap_score / total_words if total_words > 0 else 0\n",
    "\n",
    "# Test multi-vector retrieval\n",
    "retriever = MultiVectorRetriever()\n",
    "\n",
    "# Add sample documents\n",
    "docs = [\n",
    "    Document(page_content=\"Machine learning algorithms learn patterns from data to make predictions.\"),\n",
    "    Document(page_content=\"Deep learning is a subset of machine learning using neural networks.\"),\n",
    "    Document(page_content=\"Python programming is essential for data science and machine learning.\"),\n",
    "    Document(page_content=\"Natural language processing helps computers understand human language.\")\n",
    "]\n",
    "\n",
    "for doc in docs:\n",
    "    retriever.add_document(doc)\n",
    "\n",
    "# Test retrieval\n",
    "query = \"machine learning algorithms\"\n",
    "results = retriever.similarity_search(query, k=2)\n",
    "\n",
    "print(f\"üîç Multi-Vector Retrieval for: '{query}'\\n\")\n",
    "print(f\"Top {len(results)} results:\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"{i}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950ec2e0",
   "metadata": {},
   "source": [
    "## Experiment 5: Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5429a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_retrieval_methods():\n",
    "    \"\"\"Benchmark different retrieval approaches.\"\"\"\n",
    "    \n",
    "    # Simulate different methods\n",
    "    methods = {\n",
    "        'Basic Vector Search': lambda: time.sleep(0.1),\n",
    "        'Hierarchical Chunking': lambda: time.sleep(0.15),\n",
    "        'Query Decomposition': lambda: time.sleep(0.25),\n",
    "        'Contextual Compression': lambda: time.sleep(0.3),\n",
    "        'Multi-Vector': lambda: time.sleep(0.2)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for method_name, method_func in methods.items():\n",
    "        times = []\n",
    "        for _ in range(5):  # Run 5 times\n",
    "            start_time = time.time()\n",
    "            method_func()\n",
    "            times.append(time.time() - start_time)\n",
    "        \n",
    "        results[method_name] = {\n",
    "            'avg_time': np.mean(times),\n",
    "            'std_time': np.std(times)\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmark\n",
    "print(\"‚ö° Benchmarking Retrieval Methods:\\n\")\n",
    "benchmark_results = benchmark_retrieval_methods()\n",
    "\n",
    "for method, stats in benchmark_results.items():\n",
    "    print(f\"{method}:\")\n",
    "    print(f\"  Average time: {stats['avg_time']:.3f}s\")\n",
    "    print(f\"  Std deviation: {stats['std_time']:.3f}s\\n\")\n",
    "\n",
    "# Quality vs Speed analysis\n",
    "print(\"üìä Quality vs Speed Trade-offs:\")\n",
    "tradeoffs = {\n",
    "    'Basic Vector Search': {'quality': 7, 'speed': 10},\n",
    "    'Hierarchical Chunking': {'quality': 8, 'speed': 8},\n",
    "    'Query Decomposition': {'quality': 9, 'speed': 6},\n",
    "    'Contextual Compression': {'quality': 9, 'speed': 5},\n",
    "    'Multi-Vector': {'quality': 8, 'speed': 7}\n",
    "}\n",
    "\n",
    "for method, scores in tradeoffs.items():\n",
    "    efficiency = (scores['quality'] + scores['speed']) / 2\n",
    "    print(f\"{method}: Quality={scores['quality']}/10, Speed={scores['speed']}/10, Efficiency={efficiency:.1f}/10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6d314a",
   "metadata": {},
   "source": [
    "## Key Findings and Recommendations\n",
    "\n",
    "### üéØ Best Practices Discovered\n",
    "\n",
    "1. **Hierarchical Chunking**: Preserves context while enabling precise retrieval\n",
    "2. **Query Decomposition**: Improves complex query handling\n",
    "3. **Contextual Compression**: Reduces noise and improves relevance\n",
    "4. **Multi-Vector Approaches**: Combine semantic and keyword-based retrieval\n",
    "\n",
    "### üîÑ Trade-offs\n",
    "\n",
    "- **Quality vs Speed**: Advanced methods improve quality but increase latency\n",
    "- **Complexity vs Reliability**: Simple methods are more robust\n",
    "- **Cost vs Performance**: LLM-based compression is expensive but effective\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "1. Test with real datasets\n",
    "2. Implement caching for expensive operations\n",
    "3. Explore async processing for better performance\n",
    "4. Develop domain-specific optimizations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
