{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c85ffab7",
   "metadata": {},
   "source": [
    "# Advanced RAG - Retrieval Augmented Generation üîç\n",
    "\n",
    "Learn to build sophisticated RAG systems with vector databases, semantic search, and context management.\n",
    "\n",
    "## What You'll Build\n",
    "- Document ingestion pipeline\n",
    "- Vector database integration\n",
    "- Semantic search system\n",
    "- Context-aware Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdfe302",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9598f896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add utils\n",
    "sys.path.append(str(Path().parent.parent / 'utils'))\n",
    "\n",
    "try:\n",
    "    from config import get_api_key\n",
    "    api_key = get_api_key('openai')\n",
    "    print(\"‚úÖ Configuration loaded\")\n",
    "except ImportError:\n",
    "    api_key = os.getenv('OPENAI_API_KEY')\n",
    "    print(\"‚ö†Ô∏è Using basic config\")\n",
    "\n",
    "print(\"‚úÖ API key found\" if api_key else \"‚ö†Ô∏è Set OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ddc7da",
   "metadata": {},
   "source": [
    "## 2. Document Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95e79ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"LangChain is a framework for developing applications powered by language models. It enables applications that are context-aware and can reason about their environment.\",\n",
    "        metadata={\"source\": \"langchain_intro\", \"type\": \"documentation\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Vector databases store high-dimensional vectors and enable semantic search. They are crucial for RAG applications as they allow finding similar content based on meaning rather than exact keywords.\",\n",
    "        metadata={\"source\": \"vector_db_guide\", \"type\": \"technical\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Machine learning models can be fine-tuned for specific tasks. This process involves training the model on domain-specific data to improve performance on particular use cases.\",\n",
    "        metadata={\"source\": \"ml_basics\", \"type\": \"educational\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "# Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(f\"üìÑ Created {len(splits)} document chunks\")\n",
    "\n",
    "for i, split in enumerate(splits):\n",
    "    print(f\"\\nChunk {i+1}: {split.page_content[:80]}...\")\n",
    "    print(f\"Metadata: {split.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef728e23",
   "metadata": {},
   "source": [
    "## 3. Vector Store Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a52497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "if api_key:\n",
    "    # Create embeddings\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n",
    "    \n",
    "    # Create vector store\n",
    "    vectorstore = FAISS.from_documents(splits, embeddings)\n",
    "    print(\"‚úÖ Vector store created with FAISS\")\n",
    "    \n",
    "    # Test similarity search\n",
    "    query = \"What is LangChain?\"\n",
    "    docs = vectorstore.similarity_search(query, k=2)\n",
    "    \n",
    "    print(f\"\\nüîç Query: {query}\")\n",
    "    print(f\"Found {len(docs)} relevant documents:\")\n",
    "    \n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"\\n{i+1}. {doc.page_content}\")\n",
    "        print(f\"   Source: {doc.metadata.get('source', 'unknown')}\")\n",
    "else:\n",
    "    print(\"üîß Demo: Vector store would index documents for semantic search\")\n",
    "    print(\"Example: Finding documents about 'LangChain' would return relevant chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c223ef3",
   "metadata": {},
   "source": [
    "## 4. RAG Chain Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dbe1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "if api_key:\n",
    "    # Create LLM\n",
    "    llm = ChatOpenAI(\n",
    "        openai_api_key=api_key,\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    # Custom prompt for RAG\n",
    "    rag_prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=\"\"\"Use the following context to answer the question. If you don't know the answer based on the context, say so.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Create RAG chain\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vectorstore.as_retriever(search_kwargs={\"k\": 2}),\n",
    "        chain_type_kwargs={\"prompt\": rag_prompt},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ RAG chain created\")\n",
    "    \n",
    "    # Test questions\n",
    "    questions = [\n",
    "        \"What is LangChain used for?\",\n",
    "        \"How do vector databases help with search?\",\n",
    "        \"What is fine-tuning in machine learning?\"\n",
    "    ]\n",
    "    \n",
    "    for question in questions:\n",
    "        result = qa_chain({\"query\": question})\n",
    "        \n",
    "        print(f\"\\n‚ùì Q: {question}\")\n",
    "        print(f\"ü§ñ A: {result['result']}\")\n",
    "        print(f\"üìö Sources: {len(result['source_documents'])} documents used\")\n",
    "else:\n",
    "    print(\"üîß Demo: RAG chain combines retrieval with generation\")\n",
    "    print(\"Process: Question ‚Üí Retrieve relevant docs ‚Üí Generate answer with context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c6edfa",
   "metadata": {},
   "source": [
    "## 5. Advanced RAG Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d8b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "if api_key:\n",
    "    # Create conversational RAG with memory\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True\n",
    "    )\n",
    "    \n",
    "    conversational_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        memory=memory,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Conversational RAG chain created\")\n",
    "    \n",
    "    # Multi-turn conversation\n",
    "    conversation = [\n",
    "        \"What is LangChain?\",\n",
    "        \"How can it be used with vector databases?\",\n",
    "        \"Can you give me a practical example?\"\n",
    "    ]\n",
    "    \n",
    "    for i, question in enumerate(conversation, 1):\n",
    "        result = conversational_chain({\"question\": question})\n",
    "        \n",
    "        print(f\"\\n{i}. ‚ùì {question}\")\n",
    "        print(f\"   ü§ñ {result['answer'][:150]}...\")\n",
    "        \n",
    "        if i == 1:  # Only show full response for first question\n",
    "            break\n",
    "else:\n",
    "    print(\"üîß Demo: Conversational RAG maintains context across questions\")\n",
    "    print(\"Example: Follow-up questions can reference previous answers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef9a290",
   "metadata": {},
   "source": [
    "## 6. RAG Evaluation and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3072a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_response(question, answer, source_docs):\n",
    "    \"\"\"Simple RAG evaluation metrics\"\"\"\n",
    "    \n",
    "    # Relevance score (simplified)\n",
    "    question_words = set(question.lower().split())\n",
    "    answer_words = set(answer.lower().split())\n",
    "    relevance = len(question_words.intersection(answer_words)) / len(question_words)\n",
    "    \n",
    "    # Source utilization\n",
    "    source_utilization = len(source_docs) / 2  # Assuming max 2 sources\n",
    "    \n",
    "    # Answer completeness (length-based heuristic)\n",
    "    completeness = min(len(answer.split()) / 50, 1.0)  # Normalize to 50 words\n",
    "    \n",
    "    return {\n",
    "        \"relevance_score\": round(relevance, 2),\n",
    "        \"source_utilization\": round(source_utilization, 2),\n",
    "        \"completeness_score\": round(completeness, 2),\n",
    "        \"overall_score\": round((relevance + source_utilization + completeness) / 3, 2)\n",
    "    }\n",
    "\n",
    "# Example evaluation\n",
    "test_question = \"What is LangChain used for?\"\n",
    "test_answer = \"LangChain is a framework for developing applications powered by language models that are context-aware.\"\n",
    "test_sources = [\"doc1\", \"doc2\"]\n",
    "\n",
    "metrics = evaluate_rag_response(test_question, test_answer, test_sources)\n",
    "\n",
    "print(f\"üìä RAG Evaluation Metrics:\")\n",
    "for metric, score in metrics.items():\n",
    "    print(f\"   {metric}: {score}\")\n",
    "\n",
    "print(\"\\nüí° Key RAG Metrics:\")\n",
    "print(\"   ‚Ä¢ Relevance: How well the answer addresses the question\")\n",
    "print(\"   ‚Ä¢ Source Utilization: How effectively retrieved documents are used\")\n",
    "print(\"   ‚Ä¢ Completeness: Whether the answer is comprehensive\")\n",
    "print(\"   ‚Ä¢ Faithfulness: Answer accuracy to source content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e691fa1d",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "You've learned advanced RAG concepts:\n",
    "- **Document Processing**: Chunking and metadata management\n",
    "- **Vector Stores**: Semantic search with embeddings\n",
    "- **RAG Chains**: Combining retrieval with generation\n",
    "- **Conversational RAG**: Multi-turn context awareness\n",
    "- **Evaluation**: Measuring RAG system performance\n",
    "\n",
    "### Next Steps:\n",
    "1. Try different vector databases (ChromaDB, Pinecone)\n",
    "2. Experiment with chunking strategies\n",
    "3. Build domain-specific RAG systems\n",
    "4. Implement advanced retrieval techniques\n",
    "\n",
    "### Advanced Topics:\n",
    "- Multi-modal RAG (text + images)\n",
    "- Graph-based retrieval\n",
    "- Hybrid search (dense + sparse)\n",
    "- RAG optimization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510f54f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Experiment with your own RAG system\n",
    "print(\"üß™ Build your own RAG system here!\")\n",
    "print(\"Try: Different documents, chunking strategies, or retrieval methods\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
