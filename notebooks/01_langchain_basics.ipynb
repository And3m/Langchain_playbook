{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6b8ab52",
   "metadata": {},
   "source": [
    "# LangChain Basics - Interactive Tutorial ðŸ¦œðŸ”—\n",
    "\n",
    "Welcome to the interactive LangChain tutorial! This notebook covers the fundamentals in a hands-on manner.\n",
    "\n",
    "## What You'll Learn\n",
    "1. LangChain Setup & LLM Integration\n",
    "2. Prompts & Templates\n",
    "3. Chains & Memory\n",
    "4. Output Parsers\n",
    "5. Practical Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22c50b8",
   "metadata": {},
   "source": [
    "## 1. Setup and First LLM Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa34cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add utils to Python path\n",
    "sys.path.append(str(Path().parent.parent / 'utils'))\n",
    "\n",
    "try:\n",
    "    from config import get_api_key\n",
    "    print(\"âœ… Utils imported successfully\")\n",
    "except ImportError:\n",
    "    def get_api_key(provider):\n",
    "        return os.getenv(f\"{provider.upper()}_API_KEY\")\n",
    "\n",
    "api_key = get_api_key('openai')\n",
    "print(\"âœ… API key found\" if api_key else \"âš ï¸ Set OPENAI_API_KEY environment variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48b8479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "if api_key:\n",
    "    llm = ChatOpenAI(openai_api_key=api_key, temperature=0.7)\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a helpful AI assistant.\"),\n",
    "        HumanMessage(content=\"Explain LangChain in one paragraph.\")\n",
    "    ]\n",
    "    \n",
    "    response = llm(messages)\n",
    "    print(f\"ðŸ¤– AI: {response.content}\")\n",
    "else:\n",
    "    print(\"ðŸ”§ Demo: LangChain is a framework for developing applications powered by language models...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37dd074",
   "metadata": {},
   "source": [
    "## 2. Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258272f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "# Simple template\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"level\"],\n",
    "    template=\"Explain {topic} to a {level} audience.\"\n",
    ")\n",
    "\n",
    "prompt = template.format(topic=\"machine learning\", level=\"beginner\")\n",
    "print(f\"ðŸ“ Prompt: {prompt}\")\n",
    "\n",
    "# Chat template\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert {role}.\"),\n",
    "    (\"human\", \"Explain {concept} with an example.\")\n",
    "])\n",
    "\n",
    "messages = chat_template.format_messages(\n",
    "    role=\"data scientist\", \n",
    "    concept=\"neural networks\"\n",
    ")\n",
    "print(f\"\\nðŸ’¬ Chat messages: {len(messages)} messages created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927db3b8",
   "metadata": {},
   "source": [
    "## 3. Chains and Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c53e961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "if api_key:\n",
    "    # Simple chain\n",
    "    chain = LLMChain(llm=llm, prompt=template)\n",
    "    result = chain.run(topic=\"Python\", level=\"intermediate\")\n",
    "    print(f\"ðŸ”— Chain result: {result[:100]}...\")\n",
    "    \n",
    "    # Conversation with memory\n",
    "    memory = ConversationBufferMemory()\n",
    "    conversation = ConversationChain(llm=llm, memory=memory)\n",
    "    \n",
    "    response1 = conversation.predict(input=\"Hi, I'm Alex learning AI.\")\n",
    "    print(f\"\\nðŸ’­ AI: {response1[:100]}...\")\n",
    "    \n",
    "    response2 = conversation.predict(input=\"What was my name?\")\n",
    "    print(f\"ðŸ’­ AI: {response2[:100]}...\")\n",
    "else:\n",
    "    print(\"ðŸ”§ Demo: Chains combine prompts and LLMs\")\n",
    "    print(\"Memory enables contextual conversations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1ecec3",
   "metadata": {},
   "source": [
    "## 4. Output Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0305df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# List parser\n",
    "list_parser = CommaSeparatedListOutputParser()\n",
    "list_prompt = PromptTemplate(\n",
    "    template=\"List 3 {topic} concepts.\\n{format_instructions}\",\n",
    "    input_variables=[\"topic\"],\n",
    "    partial_variables={\"format_instructions\": list_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“‹ Format: {list_parser.get_format_instructions()}\")\n",
    "\n",
    "if api_key:\n",
    "    chain = LLMChain(llm=llm, prompt=list_prompt)\n",
    "    result = chain.run(topic=\"data science\")\n",
    "    parsed = list_parser.parse(result)\n",
    "    \n",
    "    print(\"\\nðŸ“Š Parsed concepts:\")\n",
    "    for i, concept in enumerate(parsed, 1):\n",
    "        print(f\"{i}. {concept.strip()}\")\n",
    "else:\n",
    "    print(\"ðŸ”§ Demo: Output parsers structure LLM responses\")\n",
    "    print(\"Example: ['Statistics', 'Machine Learning', 'Visualization']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c290e2c8",
   "metadata": {},
   "source": [
    "## 5. Practical Example: Learning Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52362f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningAssistant:\n",
    "    def __init__(self, api_key=None):\n",
    "        if api_key:\n",
    "            self.llm = ChatOpenAI(openai_api_key=api_key, temperature=0.7)\n",
    "            self.memory = ConversationBufferMemory()\n",
    "            self.conversation = ConversationChain(llm=self.llm, memory=self.memory)\n",
    "        else:\n",
    "            self.llm = None\n",
    "    \n",
    "    def ask(self, question):\n",
    "        if self.llm:\n",
    "            return self.conversation.predict(input=question)\n",
    "        else:\n",
    "            return f\"Demo response to: '{question}'\"\n",
    "\n",
    "# Create assistant\n",
    "assistant = LearningAssistant(api_key)\n",
    "print(\"ðŸŽ“ Learning Assistant Ready!\")\n",
    "\n",
    "# Demo conversation\n",
    "question = \"What is machine learning?\"\n",
    "response = assistant.ask(question)\n",
    "print(f\"\\nðŸ‘¤ {question}\")\n",
    "print(f\"ðŸŽ“ {response[:150]}...\" if len(response) > 150 else f\"ðŸŽ“ {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95727731",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "You've learned:\n",
    "- **LLM Integration**: Working with language models\n",
    "- **Prompt Templates**: Reusable, parameterized prompts\n",
    "- **Chains**: Combining operations into workflows\n",
    "- **Memory**: Adding context to conversations\n",
    "- **Output Parsing**: Structured responses\n",
    "\n",
    "### Next Steps:\n",
    "1. Explore the `/intermediate/` examples\n",
    "2. Try the `/projects/` applications\n",
    "3. Build your own LangChain app!\n",
    "\n",
    "### Resources:\n",
    "- [LangChain Docs](https://docs.langchain.com/)\n",
    "- [Example Projects](../projects/)\n",
    "- [Advanced Notebooks](./)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1efc40e",
   "metadata": {},
   "source": [
    "## ðŸ§ª Experiment Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca6e160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own experiments here!\n",
    "print(\"ðŸ§ª Create your own prompts, chains, and applications here!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
